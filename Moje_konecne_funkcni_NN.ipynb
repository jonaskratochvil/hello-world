{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moje_konecne_funkcni_NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jonaskratochvil/hello-world/blob/master/Moje_konecne_funkcni_NN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "hyuKVPe1X3PM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5oqHCBhYeD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Understanding Stochastic gradient descent, backpropagation and their implementation**\n",
        "\n",
        "**Gradient descent**\n",
        "\n",
        "The key idea in training a neural network is to minimize certain loss function, which has its imput variables\n",
        "weights and biases. As the optimization problem is non-convex we use the technique of gradient descent, more concretelly its stochastic form. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Good analogy is ball rolling down the vally, every step moving in the direction of steepest descent. The direction of steepest descent is represented by negative gradient of this cost function in a given point. We therefore need to calculate gradients of cost function with respect to all weights and biases. This is where the backpropagation comes into play.\n",
        "The stochastic aspect of gradient descent comes from the idea that rather than going through all examples and updating weights and biases after that, we update it more frequently after a small batch of data points run through network. Good analogy is that we're a drunk man going down the hill rather than carefully calculating man who takes ages to converge.\n",
        "\n",
        "**Backpropagation**\n",
        "The pseudocode for backpropagation is as follows:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DlMU0_z7g36h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Input set of training examples with true labels\n",
        "2.   for each training example: \n",
        "\n",
        "3.   feedforward for each l = 2,3,...,L : a(l) = activation_function(w_(l) dot a_(l-1) + b_(l))\n",
        "4.   output error: dL = dC/da * da_(L)\n",
        "5.   backpropagate the error: dl = (w_(l+1).T dot d(l+1) * da_(l))\n",
        "6.   update weights and biases w_in = w_in - learning_rate * (d_in dot a_(in-1).T) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CbuYgfzZja0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imagine backpropagation as going through network backwards and every step you need to open certain door with specific key. First after computing cost we need to go back to output neuron - derivative of a cost function w.r.t. output gets us in front of the neuron and to get inside we need a derivative of the activation and so on."
      ]
    },
    {
      "metadata": {
        "id": "gz9oPaq2kJqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We load the libraries that we will need."
      ]
    },
    {
      "metadata": {
        "id": "ZHZFW25xYb91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import random \n",
        "\n",
        "from scipy.stats import truncnorm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGn8-XpiX5Wi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I9Xk4PxukRUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let now comment on specific parts of object NeuralNetwork\n",
        "\n",
        "1. **init:** here we specify all important parts of the network (number of unput nodes, number of hidden nodes, number of output nodes, learning rate and we initialize weights and biases). Note the index of weights: if for example we have 3 input nodes and 4 output nodes -> our first weights, weights_in will have dimension 4 x 3.\n",
        "\n",
        "2. **train:** This is the feedforward and backpropagation part of the network. Note that we first transpose the input data so that the dimensions correspond with weights when we make the dot product. After that we proceed exactly as written in pseudo code. Note also that for this implementation we use the **Cross-entropy cost function.**  The reason for it is that if we use the L2 cost function we will have to deal with derivative of sigmoid when calculating d1, which slows the learning significantly. By introducing CE we get rig of it as the derivative of CE was specifically designed to cancel out the sigmoid prime. We save the gradients of all weights and biases\n",
        "\n",
        "3. **SGD:** We first inicialize matrices of zeros with apropriate dimensionality for all weights in/out and biases in/out. We also specify number of epochs (number of times we want to iterate whole dataset) and for each epoch we randomly shuffel the data by first writing the indexes of our dataset to an array -> randomly shuffling them and than rearanging X and y according to this shuffel. Each minibatch we accumulate the weight and bias changes and than update the weights and biases by the average of these changes time learning rate. After we run each minibatch we inicialize the matrices back to zeros and repeat over. We also track the error printing it along the way.\n",
        "\n",
        "4. **run:** Simply runs a test example to check that our neural network is giving us the desired results."
      ]
    },
    {
      "metadata": {
        "id": "M-M6o4q8YBDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  \n",
        "  def __init__(self, n_in_nodes, n_hidden_nodes, n_out_nodes, learning_rate):\n",
        "    \n",
        "    self.n_in_nodes = n_in_nodes\n",
        "    self.n_hidden_nodes = n_hidden_nodes\n",
        "    self.n_out_nodes = n_out_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weights_in = np.random.randn(n_hidden_nodes, n_in_nodes)\n",
        "    self.weights_out = np.random.randn(n_out_nodes, n_hidden_nodes)\n",
        "    self.bias_in = np.random.randn(n_hidden_nodes,1)\n",
        "    self.bias_out = np.random.randn(n_out_nodes,1)\n",
        "  \n",
        "  def train(self, input_data, input_labels):\n",
        "    \n",
        "    # Transponovani dat z 4x3 na 3x4 -> weights jsou 4x3, 1x4\n",
        "    \n",
        "    input_data = np.array(input_data, ndmin = 2).T\n",
        "    input_labels = np.array(input_labels, ndmin = 2).T\n",
        "    \n",
        "    # Forward propagate 4x3 x 3x4 -> 4x4 \n",
        "    \n",
        "    l0 = sigmoid(np.dot(self.weights_in, input_data)+self.bias_in)\n",
        "    \n",
        "    # 1x4 x 4x4 -> 1x4\n",
        "    \n",
        "    l1 = sigmoid(np.dot(self.weights_out, l0)+self.bias_out)\n",
        "    \n",
        "    # self.output_error_L2 = 1/2*(input_data - l1) **2\n",
        "    # d1 = l1 - input_data * (l1)(1-l1) -> tady je ten term derivace sigmoidu ktery spomaluje learning\n",
        "    \n",
        "    self.output_error_CE = -(input_labels * np.log(l1) + (1-input_labels) * np.log(1-l1))\n",
        "    \n",
        "    # dal jsem derivaci square loss kdyz napr 0.8 - 0 -> zaporny learning rate da to znamenko tak jak chci\n",
        "    \n",
        "    # kdyz 0.1 - 1 tak zase ten learning rate to hodi tam kam chci\n",
        "    \n",
        "    # Backpropagate -> deltas in first and zero layer from Nielsen formula\n",
        "    \n",
        "    # u d1 nyni derivace CE funkce kterou se vymaze ta derivace sigmoidu -> opravdu je videt rozdil!!!\n",
        "    \n",
        "    d1 = -input_labels*(1-l1) + (1-input_labels)*l1\n",
        "    d0 = np.dot(self.weights_out.T,d1)*(l0*(1-l0))\n",
        "    \n",
        "    # Weight update change from Nielsen formula to np.dot(delta1, output0.T)\n",
        "    \n",
        "    self.dw_out = np.dot(d1,l0.T)\n",
        "    self.dw_in = np.dot(d0,input_data.T)\n",
        "    \n",
        "    self.db_out = d1\n",
        "    self.db_in = d0\n",
        "    \n",
        "  def SGD(self, X, y, epochs, batch_size):\n",
        "    \n",
        "    batch_w_in_update = np.zeros((self.n_hidden_nodes, self.n_in_nodes), dtype=int)\n",
        "    batch_w_out_update = np.zeros((self.n_out_nodes, self.n_hidden_nodes), dtype=int)\n",
        "    batch_b_in_update = np.zeros((self.n_hidden_nodes,1), dtype=int)\n",
        "    batch_b_out_update = np.zeros((self.n_out_nodes,1), dtype=int)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "      # shuffle randomly data before start of each epoch\n",
        "      \n",
        "      data_shuffle = [i for i in range(X.shape[0])]\n",
        "      data_shuffle = random.sample(data_shuffle, len(data_shuffle))\n",
        "\n",
        "      X = np.array([X[i] for i in data_shuffle])\n",
        "      y = np.array([y[i] for i in data_shuffle])\n",
        "      \n",
        "      for minibatch in range(0,X.shape[0],batch_size):\n",
        "        \n",
        "        for example in range(minibatch,minibatch+batch_size):\n",
        "          \n",
        "          # Append weight and biases gradients\n",
        "          \n",
        "          self.train(X[example],y[example])\n",
        "          batch_w_in_update = batch_w_in_update + self.dw_in\n",
        "          batch_w_out_update = batch_w_out_update + self.dw_out\n",
        "          batch_b_in_update = batch_b_in_update + self.db_in\n",
        "          batch_b_out_update = batch_b_out_update + self.db_out\n",
        "        \n",
        "        # Update current weights to new ones with minibatch update\n",
        "        \n",
        "        self.weights_in = self.weights_in - self.learning_rate/batch_size * batch_w_in_update \n",
        "        self.weights_out = self.weights_out - self.learning_rate/batch_size * batch_w_out_update\n",
        "        \n",
        "        self.bias_in = self.bias_in - self.learning_rate/batch_size * batch_b_in_update \n",
        "        self.bias_out = self.bias_out - self.learning_rate/batch_size * batch_b_out_update\n",
        "        \n",
        "        batch_w_in_update = np.zeros((self.n_hidden_nodes, self.n_in_nodes), dtype=int)\n",
        "        batch_w_out_update = np.zeros((self.n_out_nodes, self.n_hidden_nodes), dtype=int)\n",
        "        batch_b_in_update = np.zeros((self.n_hidden_nodes,1), dtype=int)\n",
        "        batch_b_out_update = np.zeros((self.n_out_nodes,1), dtype=int)\n",
        "        \n",
        "      if (epoch % 10) == 0:\n",
        "\n",
        "        print (\"epoch:\" + str(epoch) + \" error: \" + str(np.mean(np.abs(self.output_error_CE))))\n",
        "    \n",
        "  def run(self, test_data):\n",
        "     \n",
        "    test_data = np.array(test_data, ndmin = 2).T\n",
        "    l0 = sigmoid(np.dot(self.weights_in, test_data) + self.bias_in)\n",
        "    l1 = sigmoid(np.dot(self.weights_out, l0) + self.bias_out)\n",
        "\n",
        "    return round(l1,4)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kXxfKrg_bcIV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = NeuralNetwork(3, 4, 1, 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3wAPUaublEB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array([[0,0,1],\n",
        "             [0,1,1],\n",
        "             [1,0,1],\n",
        "             [1,1,1],])\n",
        "\n",
        "y = np.array([[0,0,1,1]]).T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "byRf8R5NKlPY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "dc68cf70-e44e-4c16-8bd1-2af3fd05caa8"
      },
      "cell_type": "code",
      "source": [
        "net.SGD(X,y,200000,2)"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 error: 0.22361667150137002\n",
            "epoch:20000 error: 0.2457238359188545\n",
            "epoch:40000 error: 0.10892929600986187\n",
            "epoch:60000 error: 0.05522566845861739\n",
            "epoch:80000 error: 0.022494715098972076\n",
            "epoch:100000 error: 0.025256216365263727\n",
            "epoch:120000 error: 0.01980409301821385\n",
            "epoch:140000 error: 0.009137114922408512\n",
            "epoch:160000 error: 0.014331275752464531\n",
            "epoch:180000 error: 0.011440468759849392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tjr5-LKJYOCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(net.weights_out)\n",
        "\n",
        "print(net.weights_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ek99ndaab8JV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "728b637d-e337-4be8-ac90-d1b1e1363b47"
      },
      "cell_type": "code",
      "source": [
        "point = np.array([[1,0,1]])\n",
        "\n",
        "net.run(point)"
      ],
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99419361]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 371
        }
      ]
    },
    {
      "metadata": {
        "id": "KOcaBdUMYhCP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoJqxFaJgvhN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST.data/\", one_hot = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8VQDJwPEFG-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(mnist.train.labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTNOpTUEEPvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_mnist = NeuralNetwork(784, 10 , 10, 0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "miC632zwEyio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1a9d571-9d94-4a3b-dcd0-7ae01569a977"
      },
      "cell_type": "code",
      "source": [
        "print(mnist.test.labels[5000])"
      ],
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qv8T7sGBHfIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "9ca1d52f-3bd9-419a-b2f6-f5075827aaad"
      },
      "cell_type": "code",
      "source": [
        "net_mnist.run(mnist.test.images[5000])"
      ],
      "execution_count": 392,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.82050096e-07],\n",
              "       [2.60535144e-03],\n",
              "       [8.40765692e-03],\n",
              "       [9.28880392e-01],\n",
              "       [1.58662237e-04],\n",
              "       [5.49915933e-02],\n",
              "       [1.24461404e-06],\n",
              "       [2.41482192e-05],\n",
              "       [7.40199882e-04],\n",
              "       [6.44000153e-07]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 392
        }
      ]
    },
    {
      "metadata": {
        "id": "RfZrJy9GoYvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2e26149e-0757-4434-e83c-a5fa45435c82"
      },
      "cell_type": "code",
      "source": [
        "net_mnist.SGD(mnist.train.images,mnist.train.labels,100,10)"
      ],
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 error: 0.039886536431082344\n",
            "epoch:10 error: 0.15911996688977187\n",
            "epoch:20 error: 0.0014496427664972829\n",
            "epoch:30 error: 0.002949616116542424\n",
            "epoch:40 error: 0.04341890145081058\n",
            "epoch:50 error: 0.4786440456064943\n",
            "epoch:60 error: 0.006113164638449813\n",
            "epoch:70 error: 0.004400160056795515\n",
            "epoch:80 error: 0.0027326767033939254\n",
            "epoch:90 error: 0.004302507788731307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9vwhZ2TfqkSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}